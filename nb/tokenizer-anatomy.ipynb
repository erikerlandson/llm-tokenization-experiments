{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca0355e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1ae16",
   "metadata": {},
   "source": [
    "#### Tokenizers operate on a \"vocabulary.\"\n",
    "#### This defines the set of tokens that may be used to represent bodies of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ac424e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [\n",
    "    \" \", # alphabet\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"aa\", # other ngrams\n",
    "    \"ab\",\n",
    "    \"ac\",\n",
    "    \"aab\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047d01c",
   "metadata": {},
   "source": [
    "#### You can build a tokenizer directly from a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0925cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "tokenizer = Tokenizer(BPE())\n",
    "tokenizer.add_tokens(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2735e",
   "metadata": {},
   "source": [
    "#### Tokenizers encode text into tokens using the largest possible tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231c3c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', ' ', 'b', ' ', 'c']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"a b c\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5550f836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa', 'a', ' ', 'aab', ' ', 'aa', 'c']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"aaa aab aac\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577c8ffa",
   "metadata": {},
   "source": [
    "#### Tokenization can always fall back on base alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d47bdf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'c', 'b', 'b', 'c', 'c']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"bccbbcc\").tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ed117f",
   "metadata": {},
   "source": [
    "#### define a tokenizer that uses *only* the alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503c7bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = [\" \", \"a\", \"b\", \"c\"]\n",
    "tokenizer_alphabet = Tokenizer(BPE())\n",
    "tokenizer_alphabet.add_tokens(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3c163",
   "metadata": {},
   "source": [
    "#### define a tokenizer that includes all bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59bb564f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_bigrams = [\n",
    "    \" \", \"a\", \"b\", \"c\",\n",
    "    \"  \", \" a\", \" b\", \" c\",\n",
    "    \"a \", \"aa\", \"ab\", \"ac\",\n",
    "    \"b \", \"ba\", \"bb\", \"bc\",\n",
    "    \"c \", \"ca\", \"cb\", \"cc\",\n",
    "]\n",
    "tokenizer_bigrams = Tokenizer(BPE())\n",
    "tokenizer_bigrams.add_tokens(vocab_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eca7cb",
   "metadata": {},
   "source": [
    "#### fertility is a common definition of tokenizer compression:\n",
    "#### it is defined as the ratio of output tokens to original input length.\n",
    "#### (smaller values are generally better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6af6d6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fertility(tokenizer, corpus):\n",
    "    toks = tokenizer.encode(corpus).tokens\n",
    "    return len(toks) / len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ab365",
   "metadata": {},
   "source": [
    "#### generate a random input corpus to test fertility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2baf102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "toks = random.choices(alphabet, k=1000)\n",
    "corpus = \"\".join(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d91177a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw alphabet tokenizer has the highest fertility\n",
    "# (it is the least compressed)\n",
    "fertility(tokenizer_alphabet, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f399e19c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.844"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our original tokenizer has a moderate compression rate\n",
    "fertility(tokenizer, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aad8cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigram tokenizer has the lowest fertility\n",
    "# (it is the most compressed)\n",
    "fertility(tokenizer_bigrams, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf97eff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
