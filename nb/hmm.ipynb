{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efe817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install mchmm tokenizers datasets ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba120220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "\n",
    "def dataset_iterator(batch_size=1000):\n",
    "    tok_dataset = dataset.select_columns(\"text\")\n",
    "    diter = tok_dataset.iter(batch_size)\n",
    "    for batch in islice(diter, 1000):\n",
    "        yield batch[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ba4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import mchmm as mc\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from transformers import logging\n",
    "\n",
    "logging.enable_progress_bar()\n",
    "logging.set_verbosity_info()\n",
    "\n",
    "def hmm_train_data_iter(dataiter, tokenizer, max_batch=10):\n",
    "    for e in islice(dataiter, max_batch):\n",
    "        for txt in e:\n",
    "            for token in tokenizer.encode(txt).tokens:\n",
    "                yield token\n",
    "def hmm_train_data(dataiter, tokenizer, max_length=1000, max_batch=10):\n",
    "    return list(islice(hmm_train_data_iter(dataiter, tokenizer, max_batch), max_length))\n",
    "\n",
    "def hmm_ascii(max_batch=100, max_length=5000):\n",
    "    alphabet = [str(x) for x in string.printable]\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.add_tokens(alphabet)\n",
    "    mctrain = hmm_train_data(dataset_iterator(), tokenizer, max_batch=max_batch, max_length=max_length)\n",
    "    hmm = mc.MarkovChain().from_data(mctrain)\n",
    "    return hmm, tokenizer\n",
    "\n",
    "def run_hmm_train(ngram=1, max_batch=100, max_length=5000):\n",
    "    alphabet = [str(x) for x in string.printable]\n",
    "    trainer = BpeTrainer(max_token_length=ngram, show_progress=True, min_frequency=2, initial_alphabet=alphabet)\n",
    "    tokenizer = Tokenizer(BPE())\n",
    "    tokenizer.train_from_iterator(dataset_iterator(), trainer=trainer)\n",
    "    mctrain = hmm_train_data(dataset_iterator(), tokenizer, max_batch=max_batch, max_length=max_length)\n",
    "    hmm = mc.MarkovChain().from_data(mctrain)\n",
    "    return hmm, tokenizer\n",
    "\n",
    "def hmm_generate(hmm, n=100):\n",
    "    _, states = hmm.simulate(n)\n",
    "    return \"\".join(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_token_length does not seem to be respected, at least for ngram=1\n",
    "hmm_1, tokenizer_1 = hmm_ascii()\n",
    "hmm_5, tokenizer_5 = run_hmm_train(ngram=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af75d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f4166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_5.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a86797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' find e outh 4 thropelky Vinth geriond n blthe fin anlo tache , wotid . thiraneniedelked . pldicanec'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_generate(hmm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e482071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the Nameless are No.1 in that he deserent Valkyire , the first game some invaddition , attached . Multiple turn , but a fan rest enemy forces , along with returned to Imperform . In its predecessorporatonality in orded the public was faced by May 'n on the characters released of the battle system second overall \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_generate(hmm_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5b8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
