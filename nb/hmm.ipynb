{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1efe817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mchmm in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (0.4.5)\n",
      "Requirement already satisfied: tokenizers in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (0.21.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from mchmm) (2.3.2)\n",
      "Requirement already satisfied: scipy in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from mchmm) (1.16.1)\n",
      "Requirement already satisfied: graphviz in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from mchmm) (0.21)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from tokenizers) (0.34.4)\n",
      "Requirement already satisfied: filelock in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.7.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (1.1.8)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from datasets) (2.3.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.16.4->tokenizers)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (73 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2025.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading aiohttp-3.12.15-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.6.4-cp313-cp313-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (254 kB)\n",
      "Downloading yarl-1.20.1-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (352 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.7.0-cp313-cp313-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (232 kB)\n",
      "Downloading propcache-0.3.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyarrow-21.0.0-cp313-cp313-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K  Attempting uninstall: fsspec\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [multidict]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.7.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [multidict]\n",
      "\u001b[2K    Uninstalling fsspec-2025.7.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [multidict]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.7.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/14\u001b[0m [multidict]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14/14\u001b[0m [datasets]/14\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.15 aiosignal-1.4.0 attrs-25.3.0 datasets-4.0.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.6.4 multiprocess-0.70.16 propcache-0.3.2 pyarrow-21.0.0 xxhash-3.5.0 yarl-1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mchmm tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba120220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from itertools import islice\n",
    "\n",
    "dataset = datasets.load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"train\")\n",
    "def wikitext_iterator(batch_size=1000):\n",
    "    # Only keep the text column to avoid decoding the rest of the columns unnecessarily\n",
    "    tok_dataset = dataset.select_columns(\"text\")\n",
    "    diter = tok_dataset.iter(batch_size)\n",
    "    for batch in islice(diter, 1000):\n",
    "        yield batch[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ef45b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "trainer = BpeTrainer(max_token_length=5, show_progress=True, min_frequency=3)\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.train_from_iterator(wikitext_iterator(), trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a1ea6171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hmm_train_data_iter(dataiter, tokenizer, max_batch=10):\n",
    "    for e in islice(dataiter, max_batch):\n",
    "        for txt in e:\n",
    "            for token in tokenizer.encode(txt).tokens:\n",
    "                yield token\n",
    "def hmm_train_data(dataiter, tokenizer, max_length=1000, max_batch=10):\n",
    "    return list(islice(hmm_train_data_iter(dataiter, tokenizer, max_batch), max_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "346f7a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' = ', 'Val', 'kyri', 'a Ch', 'ron', 'ic', 'les ', 'III ', '= \\n', ' S']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hmm_train_data(wikitext_iterator(), tokenizer, max_length=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cfc765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mchmm as mc\n",
    "mctrain = hmm_train_data(wikitext_iterator(), tokenizer, max_batch=100, max_length=4000)\n",
    "hmm = mc.MarkovChain().from_data(mctrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7eee2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the battlefield in Japanded to the battle Potentire squad 422 , also unvoiced text . The player orders . Ordern early unit \" Calamity in Novements relating games , while : this escorted ment squad the battlefield map : once per echelons \\' turns . Characters . After that he designer Raita Honjou , who seeks rejected in'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, states = hmm.simulate(100)\n",
    "\"\".join(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38ba4c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import mchmm as mc\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "def run_hmm_train(ngram=1):\n",
    "    alphabet = [str(x) for x in string.printable]\n",
    "    trainer = BpeTrainer(max_token_length=ngram, show_progress=True, min_frequency=2, initial_alphabet=alphabet)\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.train_from_iterator(wikitext_iterator(), trainer=trainer)\n",
    "    mctrain = hmm_train_data(wikitext_iterator(), tokenizer, max_batch=100, max_length=4000)\n",
    "    hmm = mc.MarkovChain().from_data(mctrain)\n",
    "    return hmm, tokenizer\n",
    "\n",
    "def hmm_generate(hmm, n=100):\n",
    "    _, states = hmm.simulate(n)\n",
    "    return \"\".join(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa075c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_token_length does not seem to be respected, at least for ngram=1\n",
    "hmm_1, tokenizer_1 = run_hmm_train(ngram=1)\n",
    "hmm_3, tokenizer_3 = run_hmm_train(ngram=3)\n",
    "hmm_5, tokenizer_5 = run_hmm_train(ngram=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a86797b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e was Comediffer withe Relel tletasof cked se 's theird batter rease plauppengameirser The platials arcor Regh at offic book @-@ precria seleto gameare syso a desioffer\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_generate(hmm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59823c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"e Sect members Imperenals ' terson whifive gameplayers mation wher own ded at . \\n Taking the pers servant th echele usignerson who withe playStake missiof th occurn Blits ple p\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_generate(hmm_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e482071e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the theme of the battlefield map : once to the same times place needed by both her hics and exemplified , developed unless was release , who is , concept Irving . The Nameless \" , whose real individual @-@ specific was reture , and each characters cause certain heroinal played off to activate skills that role in the '"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_generate(hmm_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "af75d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4928"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_1.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f7f4166f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " ' , ',\n",
       " ' . ',\n",
       " ' . A',\n",
       " ' . G',\n",
       " ' = ',\n",
       " ' A',\n",
       " ' As ',\n",
       " ' Con',\n",
       " ' It ',\n",
       " ' Maj',\n",
       " ' O',\n",
       " ' Par',\n",
       " ' S',\n",
       " ' Sak',\n",
       " ' Tro',\n",
       " '\" ',\n",
       " '\" , ',\n",
       " '\" Al',\n",
       " '\" Di',\n",
       " '\" M',\n",
       " '\" N',\n",
       " '\" re',\n",
       " \"' \",\n",
       " \"'n \",\n",
       " \"'s \",\n",
       " \"'s l\",\n",
       " '( ',\n",
       " '( H',\n",
       " ') , ',\n",
       " ', ',\n",
       " ', Al',\n",
       " ', D',\n",
       " ', G',\n",
       " ', H',\n",
       " ', R',\n",
       " ', a ',\n",
       " ', ad',\n",
       " ', al',\n",
       " ', ex',\n",
       " ', h',\n",
       " ', im',\n",
       " ', s',\n",
       " ', su',\n",
       " ', wh',\n",
       " '. ',\n",
       " '. \\n',\n",
       " '. \" ',\n",
       " '. A',\n",
       " '. A ',\n",
       " '. Af',\n",
       " '. Al',\n",
       " '. Ch',\n",
       " '. D',\n",
       " '. E',\n",
       " '. Em',\n",
       " '. H',\n",
       " '. I',\n",
       " '. Li',\n",
       " '. N',\n",
       " '. O',\n",
       " '. Or',\n",
       " '. R',\n",
       " '. T',\n",
       " '. Th',\n",
       " '. W',\n",
       " '.G',\n",
       " '0 , ',\n",
       " '1 ',\n",
       " '13 ',\n",
       " '2 , ',\n",
       " '201',\n",
       " '2nd ',\n",
       " '3 ',\n",
       " '3 , ',\n",
       " '3 . ',\n",
       " '3 : ',\n",
       " '4 ',\n",
       " '4 . ',\n",
       " '42',\n",
       " '7 ',\n",
       " ': ',\n",
       " ': on',\n",
       " ': th',\n",
       " ': wh',\n",
       " '; ',\n",
       " '= ',\n",
       " '= \\n',\n",
       " '= = ',\n",
       " '@-@ ',\n",
       " 'AN',\n",
       " 'AS ',\n",
       " 'Abil',\n",
       " 'Ace ',\n",
       " 'Acti',\n",
       " 'Af',\n",
       " 'Ar',\n",
       " 'Az',\n",
       " 'B',\n",
       " 'Batt',\n",
       " 'Bli',\n",
       " 'C',\n",
       " 'Cal',\n",
       " 'Ch',\n",
       " 'Char',\n",
       " 'Comm',\n",
       " 'Crow',\n",
       " 'D',\n",
       " 'Dev',\n",
       " 'E',\n",
       " 'Emp',\n",
       " 'Eng',\n",
       " 'Euro',\n",
       " 'For',\n",
       " 'Fuji',\n",
       " 'G',\n",
       " 'Gam',\n",
       " 'Hi',\n",
       " 'Hon',\n",
       " 'I . ',\n",
       " 'II',\n",
       " 'II ',\n",
       " 'III ',\n",
       " 'Im',\n",
       " 'Irv',\n",
       " 'It ',\n",
       " 'Its ',\n",
       " 'Janu',\n",
       " 'Jap',\n",
       " 'K',\n",
       " 'Kaz',\n",
       " 'Ko',\n",
       " 'Kur',\n",
       " 'Lanc',\n",
       " 'Li',\n",
       " 'M',\n",
       " 'Mar',\n",
       " 'May ',\n",
       " 'Medi',\n",
       " 'Mil',\n",
       " 'Miy',\n",
       " 'Mus',\n",
       " 'Nag',\n",
       " 'Nam',\n",
       " 'No.',\n",
       " 'Nov',\n",
       " 'One ',\n",
       " 'P ) ',\n",
       " 'Per',\n",
       " 'Pl',\n",
       " 'Play',\n",
       " 'Po',\n",
       " 'Port',\n",
       " 'Pot',\n",
       " 'Pro',\n",
       " 'R',\n",
       " 'Ra',\n",
       " 'Rav',\n",
       " 'Re',\n",
       " 'Reg',\n",
       " 'Ri',\n",
       " 'S',\n",
       " 'Scou',\n",
       " 'Sec',\n",
       " 'Sei',\n",
       " 'Sh',\n",
       " 'Shou',\n",
       " 'Spec',\n",
       " 'Squ',\n",
       " 'St',\n",
       " 'Stor',\n",
       " 'T',\n",
       " 'Tab',\n",
       " 'Tak',\n",
       " 'The ',\n",
       " 'Un',\n",
       " 'Up ',\n",
       " 'V',\n",
       " 'Val',\n",
       " 'Vis',\n",
       " 'W',\n",
       " 'War',\n",
       " 'War ',\n",
       " 'Whi',\n",
       " 'Y',\n",
       " 'Z ',\n",
       " 'a',\n",
       " 'a ',\n",
       " 'a \" ',\n",
       " 'a , ',\n",
       " 'a : ',\n",
       " 'a Ch',\n",
       " 'a c',\n",
       " 'a f',\n",
       " 'a fi',\n",
       " 'a g',\n",
       " 'a h',\n",
       " 'a l',\n",
       " 'a li',\n",
       " 'a my',\n",
       " 'a p',\n",
       " 'a s',\n",
       " 'a st',\n",
       " 'a t',\n",
       " 'a th',\n",
       " 'a w',\n",
       " 'a.',\n",
       " 'ab',\n",
       " 'abil',\n",
       " 'ac',\n",
       " 'acc',\n",
       " 'ace ',\n",
       " 'ach',\n",
       " 'ach ',\n",
       " 'achi',\n",
       " 'ack',\n",
       " 'ack ',\n",
       " 'acqu',\n",
       " 'act',\n",
       " 'act ',\n",
       " 'acti',\n",
       " 'ad',\n",
       " 'ad ',\n",
       " 'adap',\n",
       " 'ade ',\n",
       " 'adj',\n",
       " 'adv',\n",
       " 'ady ',\n",
       " 'af',\n",
       " 'aff',\n",
       " 'aff ',\n",
       " 'ag',\n",
       " 'aha',\n",
       " 'aha ',\n",
       " 'ain',\n",
       " 'ain ',\n",
       " 'ak',\n",
       " 'ake ',\n",
       " 'al',\n",
       " 'al ',\n",
       " 'al c',\n",
       " 'alam',\n",
       " 'ale ',\n",
       " 'ali',\n",
       " 'all',\n",
       " 'all ',\n",
       " 'alli',\n",
       " 'als',\n",
       " 'als ',\n",
       " 'alw',\n",
       " 'am',\n",
       " 'am ',\n",
       " 'am w',\n",
       " 'aman',\n",
       " 'ame ',\n",
       " 'amel',\n",
       " 'amp',\n",
       " 'an',\n",
       " 'an ',\n",
       " 'an G',\n",
       " 'anci',\n",
       " 'and',\n",
       " 'and ',\n",
       " 'ang',\n",
       " 'anim',\n",
       " 'ant',\n",
       " 'ant ',\n",
       " 'ap',\n",
       " 'ap ',\n",
       " 'appe',\n",
       " 'ar',\n",
       " 'ar ',\n",
       " 'arc',\n",
       " 'ard ',\n",
       " 'are ',\n",
       " 'ari',\n",
       " 'arm',\n",
       " 'arre',\n",
       " 'ary ',\n",
       " 'as',\n",
       " 'as ',\n",
       " 'as m',\n",
       " 'ase ',\n",
       " 'ask',\n",
       " 'ass',\n",
       " 'ass ',\n",
       " 'ast',\n",
       " 'ast ',\n",
       " 'asy ',\n",
       " 'at',\n",
       " 'at ',\n",
       " 'ate ',\n",
       " 'ati',\n",
       " 'att',\n",
       " 'aty ',\n",
       " 'au',\n",
       " 'ausi',\n",
       " 'ave ',\n",
       " 'avy ',\n",
       " 'aw',\n",
       " 'awa ',\n",
       " 'awar',\n",
       " 'axim',\n",
       " 'ay',\n",
       " 'ay ',\n",
       " 'ays ',\n",
       " 'b',\n",
       " 'bas',\n",
       " 'batt',\n",
       " 'bbl',\n",
       " 'be',\n",
       " 'be ',\n",
       " 'bec',\n",
       " 'beg',\n",
       " 'bene',\n",
       " 'ber',\n",
       " 'ber ',\n",
       " 'bl',\n",
       " 'ble ',\n",
       " 'bo',\n",
       " 'boo',\n",
       " 'bu',\n",
       " 'but',\n",
       " 'but ',\n",
       " 'by ',\n",
       " 'by H',\n",
       " 'by R',\n",
       " 'by S',\n",
       " 'c',\n",
       " 'ca',\n",
       " 'ca ',\n",
       " 'can ',\n",
       " 'cap',\n",
       " 'car',\n",
       " 'ce',\n",
       " 'ce ',\n",
       " 'cell',\n",
       " 'cep',\n",
       " 'cer',\n",
       " 'cert',\n",
       " 'ces ',\n",
       " 'ch',\n",
       " 'ch ',\n",
       " 'chan',\n",
       " 'char',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'cl',\n",
       " 'clas',\n",
       " 'co',\n",
       " 'col',\n",
       " 'com',\n",
       " 'comb',\n",
       " 'comm',\n",
       " 'comp',\n",
       " 'con',\n",
       " 'cont',\n",
       " 'cor',\n",
       " 'cou',\n",
       " 'cre',\n",
       " 'cri',\n",
       " 'cur',\n",
       " 'cust',\n",
       " 'd',\n",
       " 'd ',\n",
       " 'd ab',\n",
       " 'dang',\n",
       " 'dar',\n",
       " 'de ',\n",
       " 'dec',\n",
       " 'deci',\n",
       " 'ded ',\n",
       " 'def',\n",
       " 'defe',\n",
       " 'del',\n",
       " 'deni',\n",
       " 'dep',\n",
       " 'der',\n",
       " 'der ',\n",
       " 'derw',\n",
       " 'des',\n",
       " 'desc',\n",
       " 'desp',\n",
       " 'dest',\n",
       " 'dev',\n",
       " 'di',\n",
       " 'dic',\n",
       " 'did ',\n",
       " 'diff',\n",
       " 'dist',\n",
       " 'diti',\n",
       " 'div',\n",
       " 'divi',\n",
       " 'do',\n",
       " 'do ',\n",
       " 'down',\n",
       " 'ds ',\n",
       " 'du',\n",
       " 'dual',\n",
       " 'duc',\n",
       " 'due ',\n",
       " 'dur',\n",
       " 'dy ',\n",
       " 'e',\n",
       " 'e ',\n",
       " 'e , ',\n",
       " 'e . ',\n",
       " 'e ab',\n",
       " 'e fi',\n",
       " 'e re',\n",
       " 'e th',\n",
       " 'e us',\n",
       " 'ear',\n",
       " 'ear ',\n",
       " 'eas',\n",
       " 'ec',\n",
       " 'ech ',\n",
       " 'ect ',\n",
       " 'ed ',\n",
       " 'ed G',\n",
       " 'ed K',\n",
       " 'ed S',\n",
       " 'ed f',\n",
       " 'ed t',\n",
       " 'ef',\n",
       " 'eg',\n",
       " 'ega ',\n",
       " 'eith',\n",
       " 'eiv',\n",
       " 'el',\n",
       " 'el ',\n",
       " 'ela ',\n",
       " 'eld ',\n",
       " 'elem',\n",
       " 'eles',\n",
       " 'elop',\n",
       " 'els ',\n",
       " 'ely ',\n",
       " 'em',\n",
       " 'em ',\n",
       " 'emp',\n",
       " 'emy ',\n",
       " 'en',\n",
       " 'en ',\n",
       " 'en S',\n",
       " 'en k',\n",
       " 'end',\n",
       " 'end ',\n",
       " 'enem',\n",
       " 'ener',\n",
       " 'eng',\n",
       " 'enj',\n",
       " 'ens ',\n",
       " 'ent',\n",
       " 'ent ',\n",
       " 'enti',\n",
       " 'ep ',\n",
       " 'epis',\n",
       " 'epl',\n",
       " 'ept ',\n",
       " 'er',\n",
       " 'er ',\n",
       " 'er G',\n",
       " 'er H',\n",
       " 'er f',\n",
       " 'eras',\n",
       " 'erat',\n",
       " 'ere',\n",
       " 'ere ',\n",
       " 'ern ',\n",
       " 'err',\n",
       " 'ers ',\n",
       " 'erw',\n",
       " 'es',\n",
       " 'es ',\n",
       " 'es m',\n",
       " 'es w',\n",
       " 'ese ',\n",
       " 'eshi',\n",
       " 'ess',\n",
       " 'ess ',\n",
       " 'est',\n",
       " 'est ',\n",
       " 'eth',\n",
       " 'ev',\n",
       " 'eve ',\n",
       " 'ever',\n",
       " 'evol',\n",
       " 'ew ',\n",
       " 'ex',\n",
       " 'exis',\n",
       " 'exon',\n",
       " 'exp',\n",
       " 'expl',\n",
       " 'ext ',\n",
       " 'f',\n",
       " 'f ',\n",
       " 'f ; ',\n",
       " 'fac',\n",
       " 'feat',\n",
       " 'fell',\n",
       " 'fer',\n",
       " 'fi',\n",
       " 'fin',\n",
       " 'fir',\n",
       " 'foll',\n",
       " 'for',\n",
       " 'for ',\n",
       " 'fore',\n",
       " 'form',\n",
       " 'foun',\n",
       " 'fran',\n",
       " 'fre',\n",
       " 'fron',\n",
       " 'ft ',\n",
       " 'ful',\n",
       " 'ful ',\n",
       " 'fusi',\n",
       " 'fy ',\n",
       " 'g',\n",
       " 'g ',\n",
       " 'g , ',\n",
       " 'ga ',\n",
       " 'gain',\n",
       " 'gam',\n",
       " 'gaug',\n",
       " 'ge ',\n",
       " 'get ',\n",
       " 'gett',\n",
       " 'gh',\n",
       " 'gh ',\n",
       " 'ght ',\n",
       " 'giv',\n",
       " 'go',\n",
       " 'gon ',\n",
       " 'gr',\n",
       " 'grad',\n",
       " 'gran',\n",
       " 'grap',\n",
       " 'gre',\n",
       " 'gres',\n",
       " 'gri',\n",
       " 'grow',\n",
       " 'guit',\n",
       " 'h',\n",
       " 'ha',\n",
       " 'had ',\n",
       " 'hand',\n",
       " 'happ',\n",
       " 'has ',\n",
       " 'hav',\n",
       " 'he',\n",
       " 'he ',\n",
       " 'heal',\n",
       " 'hear',\n",
       " 'hel',\n",
       " 'hen ',\n",
       " 'her ',\n",
       " 'hero',\n",
       " 'hey ',\n",
       " 'high',\n",
       " 'him',\n",
       " 'him ',\n",
       " 'his ',\n",
       " 'hou',\n",
       " 'i',\n",
       " 'i , ',\n",
       " 'i . ',\n",
       " 'ial',\n",
       " 'ial ',\n",
       " 'ian ',\n",
       " 'ic',\n",
       " 'ic ',\n",
       " 'ic b',\n",
       " 'ich ',\n",
       " 'ichi',\n",
       " 'ics ',\n",
       " 'icul',\n",
       " 'id',\n",
       " 'ide',\n",
       " 'iden',\n",
       " 'ies ',\n",
       " 'if',\n",
       " 'if ',\n",
       " 'ig',\n",
       " 'ign',\n",
       " 'ign ',\n",
       " 'il',\n",
       " 'ila ',\n",
       " 'ill',\n",
       " 'ill ',\n",
       " 'im',\n",
       " 'impe',\n",
       " 'in',\n",
       " 'in ',\n",
       " 'in G',\n",
       " 'inal',\n",
       " 'inc',\n",
       " 'inci',\n",
       " 'ine',\n",
       " 'ine ',\n",
       " 'ing',\n",
       " 'ing ',\n",
       " 'inn',\n",
       " 'int ',\n",
       " 'inu',\n",
       " 'inv',\n",
       " 'ion',\n",
       " 'ion ',\n",
       " 'ir',\n",
       " 'ir ',\n",
       " 'ird ',\n",
       " 'ire ',\n",
       " 'is ',\n",
       " 'is D',\n",
       " 'ise ',\n",
       " 'ish',\n",
       " 'ishi',\n",
       " 'ist ',\n",
       " 'it',\n",
       " 'it ',\n",
       " 'ita ',\n",
       " 'ite ',\n",
       " 'ith',\n",
       " 'ith ',\n",
       " 'iti',\n",
       " 'itor',\n",
       " 'its ',\n",
       " 'ity ',\n",
       " 'iz',\n",
       " 'j',\n",
       " 'jec',\n",
       " 'jou',\n",
       " 'jou ',\n",
       " 'k',\n",
       " 'k ',\n",
       " 'k , ',\n",
       " 'k d',\n",
       " 'k t',\n",
       " 'ke',\n",
       " 'ke ',\n",
       " 'ked ',\n",
       " 'ker ',\n",
       " 'ki ',\n",
       " 'kill',\n",
       " 'kn',\n",
       " 'knoc',\n",
       " 'know',\n",
       " 'ko ',\n",
       " 'ks ',\n",
       " 'ky',\n",
       " 'kyri',\n",
       " 'l ',\n",
       " 'lar',\n",
       " 'lat',\n",
       " 'lay',\n",
       " 'ld ',\n",
       " 'le ',\n",
       " 'lead',\n",
       " 'lear',\n",
       " 'leas',\n",
       " 'led ',\n",
       " 'lefi',\n",
       " 'legi',\n",
       " 'lem ',\n",
       " 'les ',\n",
       " 'let',\n",
       " 'leti',\n",
       " 'li',\n",
       " 'lifi',\n",
       " 'lim',\n",
       " 'lin',\n",
       " 'lo',\n",
       " 'load',\n",
       " 'loc',\n",
       " 'los',\n",
       " 'low ',\n",
       " 'ly ',\n",
       " 'ly D',\n",
       " 'ly j',\n",
       " 'ly w',\n",
       " 'm',\n",
       " 'm ',\n",
       " 'm \" ',\n",
       " 'm , ',\n",
       " 'm a ',\n",
       " 'm in',\n",
       " 'main',\n",
       " 'maj',\n",
       " 'mak',\n",
       " 'man',\n",
       " 'mand',\n",
       " 'map ',\n",
       " 'mat',\n",
       " 'mean',\n",
       " 'mech',\n",
       " 'mem',\n",
       " 'memb',\n",
       " 'ment',\n",
       " 'met ',\n",
       " 'meth',\n",
       " 'mil',\n",
       " 'min',\n",
       " 'mis',\n",
       " 'mo',\n",
       " 'mor',\n",
       " 'most',\n",
       " 'mot',\n",
       " 'mov',\n",
       " 'mul',\n",
       " 'mus',\n",
       " 'my ',\n",
       " 'n',\n",
       " 'n ',\n",
       " 'n , ',\n",
       " 'n ad',\n",
       " 'n or',\n",
       " 'n th',\n",
       " 'nam',\n",
       " 'nati',\n",
       " 'ne',\n",
       " 'ned ',\n",
       " 'nee',\n",
       " 'need',\n",
       " 'ner ',\n",
       " 'new',\n",
       " 'new ',\n",
       " 'no ',\n",
       " 'not ',\n",
       " 'ns ',\n",
       " 'num',\n",
       " 'numb',\n",
       " 'o ',\n",
       " 'o , ',\n",
       " 'o g',\n",
       " 'ob',\n",
       " 'oc',\n",
       " 'ock',\n",
       " 'od',\n",
       " 'od ',\n",
       " 'of ',\n",
       " 'of C',\n",
       " 'of D',\n",
       " 'of G',\n",
       " 'of M',\n",
       " 'of R',\n",
       " 'of T',\n",
       " 'of m',\n",
       " 'off',\n",
       " 'off ',\n",
       " 'og',\n",
       " 'oit ',\n",
       " 'ol',\n",
       " 'old ',\n",
       " 'oldi',\n",
       " 'om',\n",
       " 'ome ',\n",
       " 'omiz',\n",
       " 'on',\n",
       " 'on ',\n",
       " 'on G',\n",
       " 'on I',\n",
       " 'ond ',\n",
       " 'one ',\n",
       " 'ong',\n",
       " 'ong ',\n",
       " 'ons ',\n",
       " 'onym',\n",
       " 'ook ',\n",
       " 'op',\n",
       " 'op ',\n",
       " 'ope',\n",
       " 'open',\n",
       " 'oper',\n",
       " 'ops ',\n",
       " 'opti',\n",
       " 'opul',\n",
       " 'or',\n",
       " 'or ',\n",
       " 'orc',\n",
       " 'ord',\n",
       " 'ore ',\n",
       " 'orig',\n",
       " 'ort ',\n",
       " 'ory ',\n",
       " 'os',\n",
       " 'ose ',\n",
       " 'oshi',\n",
       " 'ost ',\n",
       " 'ot ',\n",
       " 'oth',\n",
       " 'oto ',\n",
       " 'oun',\n",
       " 'ous ',\n",
       " 'out',\n",
       " 'out ',\n",
       " 'ov',\n",
       " 'ow',\n",
       " 'ow ',\n",
       " 'owev',\n",
       " 'own ',\n",
       " 'ows ',\n",
       " 'p',\n",
       " 'p ',\n",
       " 'pan',\n",
       " 'pan ',\n",
       " 'par',\n",
       " 'peac',\n",
       " 'peak',\n",
       " 'per',\n",
       " 'per ',\n",
       " 'peri',\n",
       " 'piec',\n",
       " 'pit',\n",
       " 'pl',\n",
       " 'plat',\n",
       " 'play',\n",
       " 'ple ',\n",
       " 'ploy',\n",
       " 'po',\n",
       " 'poin',\n",
       " 'poly',\n",
       " 'por',\n",
       " 'pos',\n",
       " 'pow',\n",
       " 'pre',\n",
       " 'pres',\n",
       " 'pro',\n",
       " 'prob',\n",
       " 'proj',\n",
       " 'prot',\n",
       " 'prov',\n",
       " 'pt ',\n",
       " 'qu',\n",
       " 'que ',\n",
       " 'rag',\n",
       " 'rais',\n",
       " 'ran',\n",
       " 'rank',\n",
       " 'rath',\n",
       " 're',\n",
       " 're ',\n",
       " 'real',\n",
       " 'reas',\n",
       " 'rec',\n",
       " 'red ',\n",
       " 'rede',\n",
       " 'ree ',\n",
       " 'ref',\n",
       " 'rem',\n",
       " 'requ',\n",
       " 'res',\n",
       " 'res ',\n",
       " 'resp',\n",
       " 'ret ',\n",
       " 'ri',\n",
       " 'ribu',\n",
       " 'rid ',\n",
       " 'rit',\n",
       " 'ro',\n",
       " 'rol ',\n",
       " 'rom ',\n",
       " 'ron',\n",
       " 'rou',\n",
       " 'roy',\n",
       " 'run',\n",
       " 'ry',\n",
       " 'ry ',\n",
       " 's',\n",
       " 's ',\n",
       " 's , ',\n",
       " 's . ',\n",
       " 's re',\n",
       " 's su',\n",
       " 's th',\n",
       " 's w',\n",
       " 's wi',\n",
       " 'sal',\n",
       " 'scan',\n",
       " 'scen',\n",
       " 'scri',\n",
       " 'se',\n",
       " 'se ',\n",
       " 'seal',\n",
       " 'sec',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seg',\n",
       " 'sel',\n",
       " 'sen ',\n",
       " 'sequ',\n",
       " 'ser',\n",
       " 'serv',\n",
       " 'ses ',\n",
       " 'sett',\n",
       " 'sev',\n",
       " 'sey ',\n",
       " 'sh',\n",
       " 'shad',\n",
       " 'shar',\n",
       " 'shi',\n",
       " 'shor',\n",
       " 'si',\n",
       " 'sig',\n",
       " 'sion',\n",
       " 'sist',\n",
       " 'siti',\n",
       " 'sk',\n",
       " 'sl',\n",
       " 'so ',\n",
       " 'sol',\n",
       " 'som',\n",
       " 'son',\n",
       " 'son ',\n",
       " 'sp',\n",
       " 'spe',\n",
       " 'spec',\n",
       " 'squ',\n",
       " 'st',\n",
       " 'st ',\n",
       " 'st g',\n",
       " 'stat',\n",
       " 'ste',\n",
       " 'ster',\n",
       " 'stor',\n",
       " 'str',\n",
       " 'stri',\n",
       " 'sts ',\n",
       " 'su',\n",
       " 'sub ',\n",
       " 'succ',\n",
       " 'suff',\n",
       " 'sup',\n",
       " 'swit',\n",
       " 'sy',\n",
       " 'syst',\n",
       " 't',\n",
       " 't ',\n",
       " 't , ',\n",
       " 't . ',\n",
       " 't a ',\n",
       " 'tab',\n",
       " 'tac',\n",
       " 'tain',\n",
       " 'tak',\n",
       " 'tar',\n",
       " 'tas',\n",
       " 'tat',\n",
       " 'te',\n",
       " 'te ',\n",
       " 'ted ',\n",
       " 'tem',\n",
       " 'ten ',\n",
       " 'ter',\n",
       " 'ter ',\n",
       " 'tes ',\n",
       " 'th',\n",
       " 'th ',\n",
       " 'th G',\n",
       " 'the',\n",
       " 'the ',\n",
       " 'thel',\n",
       " 'them',\n",
       " 'thes',\n",
       " 'thou',\n",
       " 'ti',\n",
       " 'tial',\n",
       " 'tic',\n",
       " 'til ',\n",
       " 'tim',\n",
       " 'tly ',\n",
       " 'to ',\n",
       " 'to S',\n",
       " 'to e',\n",
       " 'to l',\n",
       " 'to n',\n",
       " 'to s',\n",
       " 'tor',\n",
       " 'tor ',\n",
       " 'tow',\n",
       " 'tr',\n",
       " 'tra',\n",
       " 'tran',\n",
       " 'tre',\n",
       " 'tri',\n",
       " 'tro',\n",
       " 'ts ',\n",
       " 'ts D',\n",
       " 'tun',\n",
       " 'tur',\n",
       " 'turn',\n",
       " 'two ',\n",
       " 'ty ',\n",
       " 'typ',\n",
       " 'u',\n",
       " 'u ',\n",
       " 'ual',\n",
       " 'uc',\n",
       " 'uch ',\n",
       " ...]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm_5.states.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5d629726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hmm_1.states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5b8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
