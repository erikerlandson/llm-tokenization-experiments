{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9e0516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/app-root/lib64/python3.11/site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /opt/app-root/lib64/python3.11/site-packages (1.7.0)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib64/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /opt/app-root/lib64/python3.11/site-packages (3.10.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/app-root/lib64/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in /opt/app-root/lib64/python3.11/site-packages (2.2.4)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib64/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib64/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib64/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/app-root/lib64/python3.11/site-packages (from accelerate) (2.6.0+cu126)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib64/python3.11/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/app-root/lib64/python3.11/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/app-root/lib64/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/app-root/lib64/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib64/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/app-root/lib64/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/app-root/lib64/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib64/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/app-root/lib64/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers accelerate pandas matplotlib scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f6e06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "# bits and bytes for cpu on my laptop breaks autograd\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57240761-63b2-42e6-ac33-dcf9993866fd",
   "metadata": {},
   "source": [
    "I need a checkpoint manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe02ed66-b9bb-4770-9c23-9b2e082c337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta, datetime, timezone\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "class CheckpointManager:\n",
    "    def __init__(self, basepath: str, checkpoint_interval: timedelta, max_checkpoints: int = 1):\n",
    "        self.basepath = basepath\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        self.last_checkpoint_time = datetime.now(timezone.utc)\n",
    "        self.tsformat = '%Y%m%d%H%M%S'\n",
    "        self.globexpr = f\"{self.basepath}-[0-9][0-9]*\"\n",
    "\n",
    "    def reset(self):\n",
    "        print(f\"resetting checkpoint manager for {self.basepath}\")\n",
    "        ckfiles = sorted(glob.glob(self.globexpr))\n",
    "        for fn in ckfiles:\n",
    "            if os.path.exists(fn):\n",
    "                print(f\"removing checkpoint {fn}\")\n",
    "                os.remove(fn)\n",
    "\n",
    "    def checkpoint(self, data, force: bool = False, log: str = None):\n",
    "        now = datetime.now(timezone.utc)\n",
    "        if force or ((now - self.last_checkpoint_time) >= self.checkpoint_interval):\n",
    "            ts = now.strftime(self.tsformat)\n",
    "            ckpf = f\"{self.basepath}-{ts}\"\n",
    "            print(f\"checkpoint time: {now.isoformat(sep=' ', timespec='seconds')} file: {ckpf}\")\n",
    "            if log is not None:\n",
    "                print(log)\n",
    "            with open(ckpf, \"wb\") as ckpfile:\n",
    "                pickle.dump(data, ckpfile)\n",
    "            self.last_checkpoint_time = now\n",
    "            # oldest checkpoint files are first\n",
    "            ckfiles = sorted(glob.glob(self.globexpr))\n",
    "            ndel = len(ckfiles) - self.max_checkpoints\n",
    "            if ndel > 0:\n",
    "                for fn in ckfiles[:ndel]:\n",
    "                    if os.path.exists(fn):\n",
    "                        print(f\"removing old checkpoint {fn}\")\n",
    "                        os.remove(fn)\n",
    "\n",
    "    def restore(self, ckpname = None):\n",
    "        rval = None\n",
    "        if ckpname is None:\n",
    "            ckfiles = sorted(glob.glob(self.globexpr))\n",
    "            if len(ckfiles) <= 0:\n",
    "                print(f\"found no checkpoint files with base path {self.basepath}\")\n",
    "                return None\n",
    "            ckpname = ckfiles[-1]\n",
    "        with open(ckpname, \"rb\") as ckpfile:\n",
    "            print(f\"restoring from {ckpname}\")\n",
    "            rval = pickle.load(ckpfile)\n",
    "        return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0c6f9-e50b-47f2-8c72-28c78163126c",
   "metadata": {},
   "source": [
    "### replicate a result similar to one from paper (figure 7)\n",
    "\n",
    "On the number of regions of piecewise linear neural networks\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0377042723006118?ref=pdf_download&fr=RR-2&rr=9403868f687eb6c9\n",
    "\n",
    "we expect a relatively small, countable set of affine regions, and this confirms that expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b27932f-e240-4dc9-ada1-d641af75ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting checkpoint manager for ./ckp-affine-toy\n",
      "removing checkpoint ./ckp-affine-toy-20250519212636\n",
      "removing checkpoint ./ckp-affine-toy-20250519212641\n",
      "removing checkpoint ./ckp-affine-toy-20250519212646\n",
      "checkpoint time: 2025-05-19 21:32:35+00:00 file: ./ckp-affine-toy-20250519213235\n",
      "iter: 1790 unique: 41 fingerprint: [1, 2, 3, 4, 5, 14, 15, 16, 17, 18, 20, 21, 281, 25, 29, 34, 36, 38, 39, 40, 42, 48, 180, 55, 60, 76, 82, 88, 101, 121, 123]\n",
      "checkpoint time: 2025-05-19 21:32:40+00:00 file: ./ckp-affine-toy-20250519213240\n",
      "iter: 3623 unique: 42 fingerprint: [1, 130, 385, 131, 3, 6, 7, 8, 9, 10, 11, 143, 29, 162, 34, 37, 552, 43, 45, 47, 50, 52, 58, 66, 195, 73, 80, 89, 225, 99, 100, 103, 107, 243]\n",
      "checkpoint time: 2025-05-19 21:32:45+00:00 file: ./ckp-affine-toy-20250519213245\n",
      "iter: 5429 unique: 42 fingerprint: [1, 2, 3, 5, 8, 9, 10, 13, 15, 17, 151, 280, 155, 169, 43, 44, 51, 54, 190, 191, 63, 62, 194, 71, 585, 77, 81, 852, 218, 349, 94, 98, 359, 106, 238, 110, 114, 122]\n",
      "checkpoint time: 2025-05-19 21:32:50+00:00 file: ./ckp-affine-toy-20250519213250\n",
      "iter: 7244 unique: 42 fingerprint: [256, 2, 4, 132, 1158, 6, 11, 12, 141, 270, 13, 16, 273, 145, 19, 20, 154, 798, 160, 168, 50, 312, 56, 189, 191, 67, 74, 205, 78, 86, 471, 90, 478, 95, 226, 360, 106, 246]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213235\n",
      "checkpoint time: 2025-05-19 21:32:55+00:00 file: ./ckp-affine-toy-20250519213255\n",
      "iter: 9082 unique: 42 fingerprint: [4, 7, 396, 13, 143, 15, 17, 19, 25, 282, 27, 28, 166, 174, 1458, 313, 189, 318, 63, 449, 580, 196, 73, 203, 335, 207, 83, 85, 88, 100, 358, 235, 1004, 108, 239, 113, 115, 251]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213240\n",
      "checkpoint time: 2025-05-19 21:33:00+00:00 file: ./ckp-affine-toy-20250519213300\n",
      "iter: 10910 unique: 42 fingerprint: [4, 134, 6, 136, 9, 524, 143, 16, 19, 20, 21, 407, 27, 28, 30, 289, 36, 293, 298, 432, 182, 1207, 697, 189, 704, 70, 205, 469, 87, 94, 354, 228, 229, 100, 101, 235, 1773, 113, 242, 376, 379]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213245\n",
      "checkpoint time: 2025-05-19 21:33:05+00:00 file: ./ckp-affine-toy-20250519213305\n",
      "iter: 12734 unique: 42 fingerprint: [4, 261, 6, 7, 264, 522, 267, 12, 18, 403, 148, 21, 535, 152, 23, 282, 25, 30, 31, 35, 38, 171, 434, 820, 2109, 447, 833, 338, 84, 213, 471, 218, 355, 101, 621, 237, 111, 113, 114, 1395, 127]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213250\n",
      "checkpoint time: 2025-05-19 21:33:10+00:00 file: ./ckp-affine-toy-20250519213310\n",
      "iter: 14569 unique: 42 fingerprint: [385, 130, 4, 132, 7, 14, 143, 274, 21, 23, 409, 26, 30, 927, 160, 33, 41, 42, 560, 306, 179, 308, 694, 314, 319, 961, 1604, 197, 456, 595, 597, 119, 91, 2404, 245, 247, 504, 505]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213255\n",
      "checkpoint time: 2025-05-19 21:33:15+00:00 file: ./ckp-affine-toy-20250519213315\n",
      "iter: 16400 unique: 42 fingerprint: [514, 131, 4, 1797, 646, 775, 7, 2698, 16, 145, 274, 659, 21, 149, 152, 26, 30, 35, 1061, 38, 679, 167, 557, 429, 46, 176, 48, 562, 1075, 47, 311, 449, 457, 209, 345, 347, 220, 355, 102, 360]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213300\n",
      "checkpoint time: 2025-05-19 21:33:20+00:00 file: ./ckp-affine-toy-20250519213320\n",
      "iter: 18221 unique: 42 fingerprint: [386, 515, 5, 1159, 7, 8, 402, 146, 19, 406, 25, 29, 1183, 160, 161, 33, 39, 45, 302, 174, 306, 563, 52, 51, 53, 188, 1981, 195, 3028, 340, 730, 733, 864, 225, 486, 492, 621, 109, 625, 241, 758, 376]\n",
      "removing old checkpoint ./ckp-affine-toy-20250519213305\n",
      "CPU times: user 50.1 s, sys: 1.34 s, total: 51.4 s\n",
      "Wall time: 55.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(42,\n",
       " [261,\n",
       "  7,\n",
       "  9,\n",
       "  3346,\n",
       "  23,\n",
       "  536,\n",
       "  537,\n",
       "  793,\n",
       "  153,\n",
       "  28,\n",
       "  412,\n",
       "  1309,\n",
       "  801,\n",
       "  674,\n",
       "  34,\n",
       "  677,\n",
       "  422,\n",
       "  38,\n",
       "  40,\n",
       "  559,\n",
       "  177,\n",
       "  51,\n",
       "  180,\n",
       "  53,\n",
       "  951,\n",
       "  56,\n",
       "  441,\n",
       "  60,\n",
       "  446,\n",
       "  831,\n",
       "  196,\n",
       "  333,\n",
       "  334,\n",
       "  209,\n",
       "  218,\n",
       "  1264,\n",
       "  624,\n",
       "  371,\n",
       "  247,\n",
       "  121,\n",
       "  2171])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.autograd.functional import jacobian\n",
    "toy = nn.Sequential(\n",
    "    nn.Linear(2, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1, bias=False)\n",
    ").to(\"cuda:0\")\n",
    "udist = Uniform(-2.0, 2.0)\n",
    "# this toy doesn't require checkpointing but useful for time based log outputs\n",
    "cktoy = CheckpointManager(\"./ckp-affine-toy\", timedelta(seconds = 5), max_checkpoints = 3)\n",
    "# I'm not saving randomized toy models so don't try to checkpoint restore, just nuke old files\n",
    "cktoy.reset()\n",
    "jhist = {}\n",
    "for xxx in range(20000):\n",
    "    cktoy.checkpoint(jhist, log=f\"iter: {xxx} unique: {len(jhist.values())} fingerprint: {list(set(jhist.values()))}\")\n",
    "    emb = udist.rsample(torch.Size([2])).to(\"cuda:0\")\n",
    "    jcb = jacobian(toy, emb)[0]\n",
    "    jcb = tuple(jcb.cpu().tolist())\n",
    "    # histogram of jacobians\n",
    "    if jcb not in jhist:\n",
    "        jhist[jcb] = 1\n",
    "    else:\n",
    "        fq = jhist[jcb]\n",
    "        jhist[jcb] = fq + 1\n",
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992be22-04f2-45b0-90e6-57b9fdf4a363",
   "metadata": {},
   "source": [
    "Load an LLM to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83957021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3b-code-base\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41063c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca20749d472f4675b654179c71c7edf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 2560, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (up_proj): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (down_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device \"meta\" does not load weights\n",
    "# quant = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "#    quantization_config=quant\n",
    "    ).to(\"cuda:0\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde963ed-5235-49d5-a4f4-fabe0dd6d583",
   "metadata": {},
   "source": [
    "check out the embedding function inside this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4666753",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e36d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49152, 2560])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = t[0].embed_tokens\n",
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35e1e-7bed-4aff-96be-ded3e1f4b70e",
   "metadata": {},
   "source": [
    "This is the range of actual embedding coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65a05f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8750, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(-0.3008, device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embed(torch.LongTensor(range(49000)).to(\"cuda:0\"))\n",
    "(x.max(), x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdeb3f-af5a-450e-9a6b-e6978d6e9c6a",
   "metadata": {},
   "source": [
    "The embedding logic operates on tensors of integer values. This breaks autograd and also adds an huge dimensionality (~50,000).\n",
    "\n",
    "To fix this I am creating this wrapper class whose `forward` method does an \"embedding-less\" logic, but is otherwise the same as a regular llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df31b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel\n",
    "class WrapLM(nn.Module):\n",
    "    def __init__(self, llm: LlamaModel):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.layers = llm.layers\n",
    "        self.norm = llm.norm\n",
    "        self.rotary_emb = llm.rotary_emb\n",
    "\n",
    "    def forward(self, emb: torch.Tensor) -> torch.Tensor:\n",
    "        attention_mask = None\n",
    "        position_ids = None\n",
    "        past_key_values = None\n",
    "        inputs_embeds = emb\n",
    "        use_cache = False\n",
    "        output_attentions = False\n",
    "        output_hidden_states = False\n",
    "        cache_position = None\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        cache_position = torch.arange(\n",
    "            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self.llm._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers[: self.llm.config.num_hidden_layers]:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # just taking sum to reduce dimensionality of jacobians by factor of 2500\n",
    "        rval = torch.sum(hidden_states[0][0])\n",
    "\n",
    "        return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c801b-08c3-4c73-9d10-808fffa854a2",
   "metadata": {},
   "source": [
    "wrap my llama model for use by jacobian histogramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19bc15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = WrapLM(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f91df0-a020-48c1-94e3-b5bdd268275f",
   "metadata": {},
   "source": [
    "test forward evaluation and jacobian gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6bcef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-27.6045, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embed(torch.LongTensor([[1000]]).to(\"cuda:0\"))\n",
    "wm.forward(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d222ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.2932,   5.1312,   0.7583,  ...,   6.9142,   9.0552, -18.6037]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(wm, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7796f7-1172-4f8e-90a7-9d7f15f48d37",
   "metadata": {},
   "source": [
    "Now try to replicate the discovery of affine regions on the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e293ccf5-5eff-4bec-aa44-e5b6cc9761d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring from ./ckp-reduce-xform-20250518184546\n",
      "restoring from ./ckp-affine-exp-20250519184047\n",
      "checkpoint time: 2025-05-19 21:50:20+00:00 file: ./ckp-affine-exp-20250519215020\n",
      "iter: 7264 unique: 1753322 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250519181047\n",
      "checkpoint time: 2025-05-19 22:05:20+00:00 file: ./ckp-affine-exp-20250519220520\n",
      "iter: 14530 unique: 1760588 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250519182547\n",
      "checkpoint time: 2025-05-19 22:20:20+00:00 file: ./ckp-affine-exp-20250519222020\n",
      "iter: 21806 unique: 1767864 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250519184047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m<timed exec>:16\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.autograd.functional import jacobian\n",
    "rd = 1\n",
    "udist = Uniform(0.0, 0.1)\n",
    "ckp_reduce = CheckpointManager(\"./ckp-reduce-xform\", timedelta(seconds = 30), max_checkpoints = 3)\n",
    "reduce = ckp_reduce.restore()\n",
    "if reduce is None:\n",
    "    print(f\"initializing new reduce\")\n",
    "    reduce = nn.Linear(2560, 10, bias=False).to(\"cuda:0\")\n",
    "checkpoint = CheckpointManager(\"./ckp-affine-exp\", timedelta(minutes = 15), max_checkpoints = 3)\n",
    "jhist = checkpoint.restore()\n",
    "if jhist is None:\n",
    "    print(f\"initializing new jhist\")\n",
    "    jhist = {}\n",
    "for xxx in range(100000):\n",
    "    checkpoint.checkpoint(jhist, log=f\"iter: {xxx} unique: {len(jhist.values())} fingerprint: {list(set(jhist.values()))}\")\n",
    "    emb = udist.rsample(torch.Size([1,1,2560])).to(\"cuda:0\")\n",
    "    jcb = jacobian(wm, emb)[0,0]\n",
    "    # round values to compensate for NPWL components\n",
    "    jcb = (jcb * (10 ** rd)).round() / (10 ** rd)\n",
    "    # reduce dimensionality to save space\n",
    "    # in theory this could undercount \"true\" affine regions\n",
    "    jcb = reduce(jcb)\n",
    "    # round again just because\n",
    "    jcb = (jcb * (10 ** rd)).round() / (10 ** rd)\n",
    "    # python likes tuples for keys\n",
    "    jcb = tuple(jcb.cpu().tolist())\n",
    "    # finally take the histogram\n",
    "    if jcb not in jhist:\n",
    "        jhist[jcb] = 1\n",
    "    else:\n",
    "        fq = jhist[jcb]\n",
    "        jhist[jcb] = fq + 1\n",
    "checkpoint.checkpoint(jhist, force=True)\n",
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0e2a1ca-2a38-46d0-ab74-7cc7bf1433bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring from ./ckp-affine-exp-20250519222020\n"
     ]
    }
   ],
   "source": [
    "checkpoint = CheckpointManager(\"./ckp-affine-exp\", timedelta(minutes = 30), max_checkpoints = 3)\n",
    "jhist = checkpoint.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb5ac14-cd2b-4202-aad0-8dcad03a0aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1767864, [1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b465f4-bc4c-4101-b7c2-bb1cecf4c824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
