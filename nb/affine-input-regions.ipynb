{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9e0516",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install transformers accelerate pandas matplotlib scikit-learn numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f6e06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eje/git/llm-tokenization-experiments/.venv/lib64/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import manifold\n",
    "# bits and bytes for cpu on my laptop breaks autograd\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57240761-63b2-42e6-ac33-dcf9993866fd",
   "metadata": {},
   "source": [
    "I need a checkpoint manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe02ed66-b9bb-4770-9c23-9b2e082c337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta, datetime, timezone\n",
    "import pickle\n",
    "import glob\n",
    "import os\n",
    "class CheckpointManager:\n",
    "    def __init__(self, basepath: str, checkpoint_interval: timedelta, max_checkpoints: int = 1):\n",
    "        self.basepath = basepath\n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        self.max_checkpoints = max_checkpoints\n",
    "        self.last_checkpoint_time = datetime.now(timezone.utc)\n",
    "        self.tsformat = '%Y%m%d%H%M%S'\n",
    "        self.globexpr = f\"{self.basepath}-[0-9][0-9]*\"\n",
    "\n",
    "    def reset(self):\n",
    "        print(f\"resetting checkpoint manager for {self.basepath}\")\n",
    "        ckfiles = sorted(glob.glob(self.globexpr))\n",
    "        for fn in ckfiles:\n",
    "            if os.path.exists(fn):\n",
    "                print(f\"removing checkpoint {fn}\")\n",
    "                os.remove(fn)\n",
    "\n",
    "    def checkpoint(self, data, force: bool = False, log: str = None):\n",
    "        now = datetime.now(timezone.utc)\n",
    "        if force or ((now - self.last_checkpoint_time) >= self.checkpoint_interval):\n",
    "            ts = now.strftime(self.tsformat)\n",
    "            ckpf = f\"{self.basepath}-{ts}\"\n",
    "            print(f\"checkpoint time: {now.isoformat(sep=' ', timespec='seconds')} file: {ckpf}\")\n",
    "            if log is not None:\n",
    "                print(log)\n",
    "            with open(ckpf, \"wb\") as ckpfile:\n",
    "                pickle.dump(data, ckpfile)\n",
    "            self.last_checkpoint_time = now\n",
    "            # oldest checkpoint files are first\n",
    "            ckfiles = sorted(glob.glob(self.globexpr))\n",
    "            ndel = len(ckfiles) - self.max_checkpoints\n",
    "            if ndel > 0:\n",
    "                for fn in ckfiles[:ndel]:\n",
    "                    if os.path.exists(fn):\n",
    "                        print(f\"removing old checkpoint {fn}\")\n",
    "                        os.remove(fn)\n",
    "\n",
    "    def restore(self, ckpname = None):\n",
    "        rval = None\n",
    "        if ckpname is None:\n",
    "            ckfiles = sorted(glob.glob(self.globexpr))\n",
    "            if len(ckfiles) <= 0:\n",
    "                print(f\"found no checkpoint files with base path {self.basepath}\")\n",
    "                return None\n",
    "            ckpname = ckfiles[-1]\n",
    "        with open(ckpname, \"rb\") as ckpfile:\n",
    "            print(f\"restoring from {ckpname}\")\n",
    "            rval = pickle.load(ckpfile)\n",
    "        return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0c6f9-e50b-47f2-8c72-28c78163126c",
   "metadata": {},
   "source": [
    "### replicate a result similar to one from paper (figure 7)\n",
    "\n",
    "On the number of regions of piecewise linear neural networks\n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0377042723006118?ref=pdf_download&fr=RR-2&rr=9403868f687eb6c9\n",
    "\n",
    "we expect a relatively small, countable set of affine regions, and this confirms that expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b27932f-e240-4dc9-ada1-d641af75ac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resetting checkpoint manager for ./ckp-affine-toy\n",
      "removing checkpoint ./ckp-affine-toy-20250820213429\n",
      "checkpoint time: 2025-08-22 13:18:23+00:00 file: ./ckp-affine-toy-20250822131823\n",
      "iter: 14013 unique: 40 fingerprint: [387, 1029, 392, 520, 136, 20, 284, 542, 286, 34, 36, 426, 303, 48, 945, 307, 437, 568, 196, 68, 70, 327, 72, 337, 1620, 348, 101, 487, 104, 234, 236, 1005, 239, 372, 373, 118, 117, 251, 127, 511]\n",
      "CPU times: user 6.96 s, sys: 440 ms, total: 7.4 s\n",
      "Wall time: 7.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40,\n",
       " [2308,\n",
       "  523,\n",
       "  268,\n",
       "  398,\n",
       "  147,\n",
       "  148,\n",
       "  534,\n",
       "  27,\n",
       "  676,\n",
       "  555,\n",
       "  428,\n",
       "  43,\n",
       "  174,\n",
       "  175,\n",
       "  432,\n",
       "  48,\n",
       "  827,\n",
       "  191,\n",
       "  1472,\n",
       "  71,\n",
       "  1480,\n",
       "  1362,\n",
       "  338,\n",
       "  469,\n",
       "  728,\n",
       "  347,\n",
       "  609,\n",
       "  354,\n",
       "  610,\n",
       "  100,\n",
       "  486,\n",
       "  743,\n",
       "  104,\n",
       "  105,\n",
       "  363,\n",
       "  493,\n",
       "  753,\n",
       "  504])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.autograd.functional import jacobian\n",
    "toy = nn.Sequential(\n",
    "    nn.Linear(2, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 1, bias=False)\n",
    ").to(\"cuda:0\")\n",
    "udist = Uniform(-2.0, 2.0)\n",
    "# this toy doesn't require checkpointing but useful for time based log outputs\n",
    "cktoy = CheckpointManager(\"./ckp-affine-toy\", timedelta(seconds = 5), max_checkpoints = 3)\n",
    "# I'm not saving randomized toy models so don't try to checkpoint restore, just nuke old files\n",
    "cktoy.reset()\n",
    "jhist = {}\n",
    "for xxx in range(20000):\n",
    "    cktoy.checkpoint(jhist, log=f\"iter: {xxx} unique: {len(jhist.values())} fingerprint: {list(set(jhist.values()))}\")\n",
    "    emb = udist.rsample(torch.Size([2])).to(\"cuda:0\")\n",
    "    jcb = jacobian(toy, emb)[0]\n",
    "    jcb = tuple(jcb.cpu().tolist())\n",
    "    # histogram of jacobians\n",
    "    if jcb not in jhist:\n",
    "        jhist[jcb] = 1\n",
    "    else:\n",
    "        fq = jhist[jcb]\n",
    "        jhist[jcb] = fq + 1\n",
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992be22-04f2-45b0-90e6-57b9fdf4a363",
   "metadata": {},
   "source": [
    "Load an LLM to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83957021",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-3b-code-base\"\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41063c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 2560, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (o_proj): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (up_proj): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (down_proj): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2560,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device \"meta\" does not load weights\n",
    "# quant = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "#    quantization_config=quant\n",
    "    ).to(\"cuda:0\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde963ed-5235-49d5-a4f4-fabe0dd6d583",
   "metadata": {},
   "source": [
    "check out the embedding function inside this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4666753",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e36d80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49152, 2560])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = t[0].embed_tokens\n",
    "embed.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35e1e-7bed-4aff-96be-ded3e1f4b70e",
   "metadata": {},
   "source": [
    "This is the range of actual embedding coordinate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a05f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8750, device='cuda:0', grad_fn=<MaxBackward1>),\n",
       " tensor(-0.3008, device='cuda:0', grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embed(torch.LongTensor(range(49000)).to(\"cuda:0\"))\n",
    "(x.max(), x.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecdeb3f-af5a-450e-9a6b-e6978d6e9c6a",
   "metadata": {},
   "source": [
    "The embedding logic operates on tensors of integer values. This breaks autograd and also adds an huge dimensionality (~50,000).\n",
    "\n",
    "To fix this I am creating this wrapper class whose `forward` method does an \"embedding-less\" logic, but is otherwise the same as a regular llama model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df31b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel\n",
    "from transformers.masking_utils import create_causal_mask\n",
    "class WrapLM(nn.Module):\n",
    "    def __init__(self, llm: LlamaModel):\n",
    "        super().__init__()\n",
    "        self.llm = llm\n",
    "        self.layers = llm.layers\n",
    "        self.norm = llm.norm\n",
    "        self.rotary_emb = llm.rotary_emb\n",
    "        self.config = llm.config\n",
    "\n",
    "    def forward(self, emb: torch.Tensor) -> torch.Tensor:\n",
    "        input_ids = None\n",
    "        attention_mask = None\n",
    "        position_ids = None\n",
    "        past_key_values = None\n",
    "        inputs_embeds = emb\n",
    "        cache_position = None\n",
    "        use_cache = False\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache(config=self.config)\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position: torch.Tensor = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = create_causal_mask(\n",
    "            config=self.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_values=past_key_values,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        # just taking sum to reduce dimensionality of jacobians by factor of 2500\n",
    "        rval = torch.sum(hidden_states[0][0])\n",
    "        return rval\n",
    "    \n",
    "\n",
    "    def old_forward(self, emb: torch.Tensor) -> torch.Tensor:\n",
    "        attention_mask = None\n",
    "        position_ids = None\n",
    "        past_key_values = None\n",
    "        inputs_embeds = emb\n",
    "        use_cache = False\n",
    "        output_attentions = False\n",
    "        output_hidden_states = False\n",
    "        cache_position = None\n",
    "\n",
    "        past_seen_tokens = 0\n",
    "        cache_position = torch.arange(\n",
    "            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "        )\n",
    "\n",
    "        position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        # causal_mask = self.llm._update_causal_mask(\n",
    "        #    attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        #)\n",
    "\n",
    "        causal_mask = create_causal_mask(\n",
    "            config=self.llm.config,\n",
    "            input_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            cache_position=cache_position,\n",
    "            past_key_values=past_key_values,\n",
    "            position_ids=position_ids\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        for decoder_layer in self.layers[: self.llm.config.num_hidden_layers]:\n",
    "            layer_outputs = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask,\n",
    "                position_ids=position_ids,\n",
    "                past_key_value=past_key_values,\n",
    "                output_attentions=output_attentions,\n",
    "                use_cache=use_cache,\n",
    "                cache_position=cache_position,\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # just taking sum to reduce dimensionality of jacobians by factor of 2500\n",
    "        rval = torch.sum(hidden_states[0][0])\n",
    "\n",
    "        return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c801b-08c3-4c73-9d10-808fffa854a2",
   "metadata": {},
   "source": [
    "wrap my llama model for use by jacobian histogramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19bc15db",
   "metadata": {},
   "outputs": [],
   "source": [
    "wm = WrapLM(model.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f91df0-a020-48c1-94e3-b5bdd268275f",
   "metadata": {},
   "source": [
    "test forward evaluation and jacobian gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6bcef8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-27.6045, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = embed(torch.LongTensor([[1000]]).to(\"cuda:0\"))\n",
    "wm.forward(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d222ded8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -1.2932,   5.1312,   0.7583,  ...,   6.9142,   9.0552, -18.6037]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.functional.jacobian(wm, emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7796f7-1172-4f8e-90a7-9d7f15f48d37",
   "metadata": {},
   "source": [
    "Now try to replicate the discovery of affine regions on the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e293ccf5-5eff-4bec-aa44-e5b6cc9761d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found no checkpoint files with base path ./ckp-reduce-xform\n",
      "initializing new reduce\n",
      "restoring from ./ckp-affine-exp-20250824003918\n",
      "checkpoint time: 2025-08-24 02:43:41+00:00 file: ./ckp-affine-exp-20250824024341\n",
      "iter: 5346 unique: 507346 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824002252\n",
      "checkpoint time: 2025-08-24 02:58:42+00:00 file: ./ckp-affine-exp-20250824025842\n",
      "iter: 9847 unique: 511847 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824003752\n",
      "checkpoint time: 2025-08-24 03:13:42+00:00 file: ./ckp-affine-exp-20250824031342\n",
      "iter: 14350 unique: 516350 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824003918\n",
      "checkpoint time: 2025-08-24 03:28:42+00:00 file: ./ckp-affine-exp-20250824032842\n",
      "iter: 18848 unique: 520848 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824024341\n",
      "checkpoint time: 2025-08-24 03:43:42+00:00 file: ./ckp-affine-exp-20250824034342\n",
      "iter: 23401 unique: 525401 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824025842\n",
      "checkpoint time: 2025-08-24 03:58:42+00:00 file: ./ckp-affine-exp-20250824035842\n",
      "iter: 27912 unique: 529912 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824031342\n",
      "checkpoint time: 2025-08-24 04:13:42+00:00 file: ./ckp-affine-exp-20250824041342\n",
      "iter: 32418 unique: 534418 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824032842\n",
      "checkpoint time: 2025-08-24 04:28:42+00:00 file: ./ckp-affine-exp-20250824042842\n",
      "iter: 36926 unique: 538926 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824034342\n",
      "checkpoint time: 2025-08-24 04:43:43+00:00 file: ./ckp-affine-exp-20250824044343\n",
      "iter: 41438 unique: 543438 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824035842\n",
      "checkpoint time: 2025-08-24 04:58:43+00:00 file: ./ckp-affine-exp-20250824045843\n",
      "iter: 45977 unique: 547977 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824041342\n",
      "checkpoint time: 2025-08-24 05:13:43+00:00 file: ./ckp-affine-exp-20250824051343\n",
      "iter: 50520 unique: 552520 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824042842\n",
      "checkpoint time: 2025-08-24 05:28:43+00:00 file: ./ckp-affine-exp-20250824052843\n",
      "iter: 55066 unique: 557066 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824044343\n",
      "checkpoint time: 2025-08-24 05:43:43+00:00 file: ./ckp-affine-exp-20250824054343\n",
      "iter: 59624 unique: 561624 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824045843\n",
      "checkpoint time: 2025-08-24 05:58:43+00:00 file: ./ckp-affine-exp-20250824055843\n",
      "iter: 64223 unique: 566223 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824051343\n",
      "checkpoint time: 2025-08-24 06:13:43+00:00 file: ./ckp-affine-exp-20250824061343\n",
      "iter: 68896 unique: 570896 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824052843\n",
      "checkpoint time: 2025-08-24 06:28:43+00:00 file: ./ckp-affine-exp-20250824062843\n",
      "iter: 73680 unique: 575680 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824054343\n",
      "checkpoint time: 2025-08-24 06:43:43+00:00 file: ./ckp-affine-exp-20250824064343\n",
      "iter: 78444 unique: 580444 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824055843\n",
      "checkpoint time: 2025-08-24 06:58:43+00:00 file: ./ckp-affine-exp-20250824065843\n",
      "iter: 83208 unique: 585208 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824061343\n",
      "checkpoint time: 2025-08-24 07:13:44+00:00 file: ./ckp-affine-exp-20250824071344\n",
      "iter: 88084 unique: 590084 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824062843\n",
      "checkpoint time: 2025-08-24 07:28:44+00:00 file: ./ckp-affine-exp-20250824072844\n",
      "iter: 93064 unique: 595064 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824064343\n",
      "checkpoint time: 2025-08-24 07:43:44+00:00 file: ./ckp-affine-exp-20250824074344\n",
      "iter: 98047 unique: 600047 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824065843\n",
      "checkpoint time: 2025-08-24 07:58:44+00:00 file: ./ckp-affine-exp-20250824075844\n",
      "iter: 103104 unique: 605104 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824071344\n",
      "checkpoint time: 2025-08-24 08:13:44+00:00 file: ./ckp-affine-exp-20250824081344\n",
      "iter: 108147 unique: 610147 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824072844\n",
      "checkpoint time: 2025-08-24 08:28:44+00:00 file: ./ckp-affine-exp-20250824082844\n",
      "iter: 113311 unique: 615311 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824074344\n",
      "checkpoint time: 2025-08-24 08:43:44+00:00 file: ./ckp-affine-exp-20250824084344\n",
      "iter: 118457 unique: 620457 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824075844\n",
      "checkpoint time: 2025-08-24 08:58:44+00:00 file: ./ckp-affine-exp-20250824085844\n",
      "iter: 123728 unique: 625728 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824081344\n",
      "checkpoint time: 2025-08-24 09:13:44+00:00 file: ./ckp-affine-exp-20250824091344\n",
      "iter: 129003 unique: 631003 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824082844\n",
      "checkpoint time: 2025-08-24 09:28:44+00:00 file: ./ckp-affine-exp-20250824092844\n",
      "iter: 134384 unique: 636384 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824084344\n",
      "checkpoint time: 2025-08-24 09:43:45+00:00 file: ./ckp-affine-exp-20250824094345\n",
      "iter: 139769 unique: 641769 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824085844\n",
      "checkpoint time: 2025-08-24 09:58:45+00:00 file: ./ckp-affine-exp-20250824095845\n",
      "iter: 145249 unique: 647249 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824091344\n",
      "checkpoint time: 2025-08-24 10:13:45+00:00 file: ./ckp-affine-exp-20250824101345\n",
      "iter: 150732 unique: 652732 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824092844\n",
      "checkpoint time: 2025-08-24 10:28:45+00:00 file: ./ckp-affine-exp-20250824102845\n",
      "iter: 156210 unique: 658210 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824094345\n",
      "checkpoint time: 2025-08-24 10:43:45+00:00 file: ./ckp-affine-exp-20250824104345\n",
      "iter: 161794 unique: 663794 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824095845\n",
      "checkpoint time: 2025-08-24 10:58:45+00:00 file: ./ckp-affine-exp-20250824105845\n",
      "iter: 167210 unique: 669210 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824101345\n",
      "checkpoint time: 2025-08-24 11:13:45+00:00 file: ./ckp-affine-exp-20250824111345\n",
      "iter: 172606 unique: 674606 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824102845\n",
      "checkpoint time: 2025-08-24 11:28:45+00:00 file: ./ckp-affine-exp-20250824112845\n",
      "iter: 178033 unique: 680033 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824104345\n",
      "checkpoint time: 2025-08-24 11:43:45+00:00 file: ./ckp-affine-exp-20250824114345\n",
      "iter: 183524 unique: 685524 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824105845\n",
      "checkpoint time: 2025-08-24 11:58:45+00:00 file: ./ckp-affine-exp-20250824115845\n",
      "iter: 189015 unique: 691015 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824111345\n",
      "checkpoint time: 2025-08-24 12:13:45+00:00 file: ./ckp-affine-exp-20250824121345\n",
      "iter: 194257 unique: 696257 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824112845\n",
      "checkpoint time: 2025-08-24 12:28:46+00:00 file: ./ckp-affine-exp-20250824122846\n",
      "iter: 199685 unique: 701685 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824114345\n",
      "checkpoint time: 2025-08-24 12:43:46+00:00 file: ./ckp-affine-exp-20250824124346\n",
      "iter: 205105 unique: 707105 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824115845\n",
      "checkpoint time: 2025-08-24 12:58:46+00:00 file: ./ckp-affine-exp-20250824125846\n",
      "iter: 210498 unique: 712498 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824121345\n",
      "checkpoint time: 2025-08-24 13:13:46+00:00 file: ./ckp-affine-exp-20250824131346\n",
      "iter: 215895 unique: 717895 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824122846\n",
      "checkpoint time: 2025-08-24 13:28:46+00:00 file: ./ckp-affine-exp-20250824132846\n",
      "iter: 221356 unique: 723356 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824124346\n",
      "checkpoint time: 2025-08-24 13:43:46+00:00 file: ./ckp-affine-exp-20250824134346\n",
      "iter: 226939 unique: 728939 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824125846\n",
      "checkpoint time: 2025-08-24 13:58:46+00:00 file: ./ckp-affine-exp-20250824135846\n",
      "iter: 232698 unique: 734698 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824131346\n",
      "checkpoint time: 2025-08-24 14:13:46+00:00 file: ./ckp-affine-exp-20250824141346\n",
      "iter: 238772 unique: 740772 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824132846\n",
      "checkpoint time: 2025-08-24 14:28:46+00:00 file: ./ckp-affine-exp-20250824142846\n",
      "iter: 244781 unique: 746781 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824134346\n",
      "checkpoint time: 2025-08-24 14:43:46+00:00 file: ./ckp-affine-exp-20250824144346\n",
      "iter: 250840 unique: 752840 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824135846\n",
      "checkpoint time: 2025-08-24 14:58:46+00:00 file: ./ckp-affine-exp-20250824145846\n",
      "iter: 256865 unique: 758865 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824141346\n",
      "checkpoint time: 2025-08-24 15:13:46+00:00 file: ./ckp-affine-exp-20250824151346\n",
      "iter: 262823 unique: 764823 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824142846\n",
      "checkpoint time: 2025-08-24 15:28:47+00:00 file: ./ckp-affine-exp-20250824152847\n",
      "iter: 268737 unique: 770737 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824144346\n",
      "checkpoint time: 2025-08-24 15:43:47+00:00 file: ./ckp-affine-exp-20250824154347\n",
      "iter: 274613 unique: 776613 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824145846\n",
      "checkpoint time: 2025-08-24 15:58:47+00:00 file: ./ckp-affine-exp-20250824155847\n",
      "iter: 280341 unique: 782341 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824151346\n",
      "checkpoint time: 2025-08-24 16:13:47+00:00 file: ./ckp-affine-exp-20250824161347\n",
      "iter: 286128 unique: 788128 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824152847\n",
      "checkpoint time: 2025-08-24 16:28:47+00:00 file: ./ckp-affine-exp-20250824162847\n",
      "iter: 291947 unique: 793947 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824154347\n",
      "checkpoint time: 2025-08-24 16:43:47+00:00 file: ./ckp-affine-exp-20250824164347\n",
      "iter: 297551 unique: 799551 fingerprint: [1]\n",
      "removing old checkpoint ./ckp-affine-exp-20250824155847\n",
      "checkpoint time: 2025-08-24 16:50:22+00:00 file: ./ckp-affine-exp-20250824165022\n",
      "removing old checkpoint ./ckp-affine-exp-20250824161347\n",
      "CPU times: user 9h 58min 35s, sys: 4h 20min 45s, total: 14h 19min 21s\n",
      "Wall time: 14h 21min 40s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(802000, [1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch.autograd.functional import jacobian\n",
    "rd = 1\n",
    "udist = Uniform(0.0, 0.1)\n",
    "ckp_reduce = CheckpointManager(\"./ckp-reduce-xform\", timedelta(seconds = 30), max_checkpoints = 3)\n",
    "reduce = ckp_reduce.restore()\n",
    "if reduce is None:\n",
    "    print(f\"initializing new reduce\")\n",
    "    reduce = nn.Linear(2560, 10, bias=False).to(\"cuda:0\")\n",
    "checkpoint = CheckpointManager(\"./ckp-affine-exp\", timedelta(minutes = 15), max_checkpoints = 3)\n",
    "jhist = checkpoint.restore()\n",
    "if jhist is None:\n",
    "    print(f\"initializing new jhist\")\n",
    "    jhist = {}\n",
    "for xxx in range(300000):\n",
    "    checkpoint.checkpoint(jhist, log=f\"iter: {xxx} unique: {len(jhist.values())} fingerprint: {list(set(jhist.values()))}\")\n",
    "    emb = udist.rsample(torch.Size([1,1,2560])).to(\"cuda:0\")\n",
    "    jcb = jacobian(wm, emb)[0,0]\n",
    "    # round values to compensate for NPWL components\n",
    "    jcb = (jcb * (10 ** rd)).round() / (10 ** rd)\n",
    "    # reduce dimensionality to save space\n",
    "    # in theory this could undercount \"true\" affine regions\n",
    "    jcb = reduce(jcb)\n",
    "    # round again just because\n",
    "    jcb = (jcb * (10 ** rd)).round() / (10 ** rd)\n",
    "    # python likes tuples for keys\n",
    "    jcb = tuple(jcb.cpu().tolist())\n",
    "    # finally take the histogram\n",
    "    if jcb not in jhist:\n",
    "        jhist[jcb] = 1\n",
    "    else:\n",
    "        fq = jhist[jcb]\n",
    "        jhist[jcb] = fq + 1\n",
    "checkpoint.checkpoint(jhist, force=True)\n",
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abb5ac14-cd2b-4202-aad0-8dcad03a0aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring from ./ckp-affine-exp-20250822175002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(202000, [1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = CheckpointManager(\"./ckp-affine-exp\", timedelta(minutes = 30), max_checkpoints = 3)\n",
    "jhist = checkpoint.restore()\n",
    "(len(jhist.values()), list(set(jhist.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b465f4-bc4c-4101-b7c2-bb1cecf4c824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
